bagging
  bootstrap aggregating
  주어진 훈련 데이터에서 중복을 허용하여 원 데이터셋과 같은 크기의 데이터셋을 만듦
  트리들의 편향은 그대로 유지, 분산은 감소시켜 성능 향상
  트리들이 서로 상관화correlated 되어 있지 않다면 트리들의 평균은 노이즈에 강인
  
bagging 훈련과정
1. 부트스트랩으로 T개의 훈련 데이터셋 생성
2. T개의 트리들을 훈련
3. 트리들을 랜덤포레스트로 집계 (평균/과반수)

앙상블 방법
  여러 방법을 섞어 사용
 
트리의 깊이
  시작 노드 제외

Random Forest
  앙상블 방법
  
kaggle bike sharing demand
RMLSE = /‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾
       √ 1/𝑛∑(log(𝑝𝑖+1)−log(𝑎𝑖+1))^2
-∞가 되지 않게 +1
log1p


df를 train, tail로 나누기
1) df.loc[df["count"].notnull(), feature_names]
2) df.iloc[train.index][feature_names]

  평가
cross_validate(scoring="")
sklearn.metrics.SCORERS.keys()에서 찾은
평가 지표로 score을 확인할 수 있다.(y_predict는 y_valid_predict)

from sklearn.metrics import mean_absolute_error
mean_absolute_error(y_train, y_valid_predict)





