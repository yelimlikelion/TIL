딥러닝..

인공 신경망 ANN
  인간의 두뇌를 본따 만들어짐
퍼셉트론
  입력 신호에 따라 가중치 부여
  가중치의 합이 임계값에 도달하면 1
  단층 퍼셉트론, 다층 퍼셉트론(hidden layer)
역전파
  다층 퍼셉트론 학습에 사용하는 통계적 기법
  가중치 조정 오차 역전법
  
활성화 함수
  - 시그모이드
      이진 분류
      모델 제작 빠름 vs 정보가 손실
      tanh와 비교하여 느리지만 세밀하게 학습
  - tanh
      상대적으로 빠름
  기울기 소실(Gradient Vanishing)
  - ReLU ✔︎
      0 이하에서 0
  - Leaky ReLU
      0 이하에서 아주 작은 음수 
딥러닝의 학습방법
  측정값과 실제값을 비교 → 차이를 최소화하는 가중치와 편향 조합
손실 함수
  실제값과 예측값의 차이를 수치화
모델에 따라 오차 측정기법 다름
분류 : Cross Entropy Loss
  -(ylog(p) + (1-y)log(1-p))
  낮을수록 좋음

경사하강법
  손실함수의 현재 위치에서 손실이 낮아지는 쪽으로 가중치를 움직여 최소값을 찾음
최적화 함수(Optimizer)
  손실함수를 최소화하는 방향으로 가중치를 갱신
  
DNN(Deep Neural Network)
  ANN 기반으로 Hidden layer을 늘려서 학습 결과 향상
  높은 시간복잡도 → mini batch

층 구성 → 컴파일 → 요약 → 학습 → 예측 → 평가

출력층에서 활성화 함수
  분류  sigmoid(이진), softmax(다중 클래스)  모든 입력신호로부터 영향
  회귀  identity function 계산된 값 그대로 

MNIST 손글씨 분류
  층 구성
    2차원 행렬을 1차원 형태로 flatten(input_shape=())
    units 개수만큼 노드를 가지고 연결 Dense(units=, activation='relu') 
    과적합 피하기 위해 Dropout
  컴파일
    optimizer(손실함수의 값을 낮추는 매개변수)
    loss(손실함수)
    metrics
  요약
    param

sparse_categorical_crossentropy   label 정수형태
categorical_crossentropy          label이 one-hot vector
